<html>
  <head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default"></script>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <style>
      h1 {
        text-align: center;
      }

      .container {
        margin: 0 auto;
        padding: 60px 20%;
      }

      figure {
        text-align: center;
      }

      img {
        display: inline-block;
      }

      body {
        font-family: "Inter", sans-serif;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>CS184/284A Spring 2025 Homework 1 Write-Up</h1>
      <div style="text-align: center">Names: <!-- TODO: Fill in your name(s) --></div>

      <br />

      <div>
        Link to webpage: (TODO)
        <a href="https://cs184.eecs.berkeley.edu/sp25">cs184.eecs.berkeley.edu/sp25</a>
      </div>

      <br />

      <div>
        Link to GitHub repository: (TODO)
        <a href="https://cs184.eecs.berkeley.edu/sp25">cs184.eecs.berkeley.edu/sp25</a>
      </div>

      <br />

      <h2>Overview</h2>
      <p>
        In this homework I implemented a small but complete 2D software rasterizer that takes SVG
        scenes and converts them into pixels using triangle rasterization, supersampling for
        antialiasing, barycentric interpolation, and textured sampling with mipmaps. The pipeline
        goes from SVG primitives through transforms into a supersample buffer and finally into an
        RGB framebuffer.
      </p>
      <p>
        The most interesting parts were (1) understanding how to turn continuous geometry into
        discrete samples using edge functions and bounding boxes, (2) seeing directly how
        supersampling and filtering trade off sharpness for smoothness to fight aliasing, and (3)
        using derivatives of texture coordinates to select appropriate mipmap levels. Putting all of
        these together made the abstract ideas from lecture feel concrete, since I could see how
        each design decision changed actual rendered images.
      </p>

      <h2>Task 1: Drawing Single-Color Triangles</h2>
      <p>
        To rasterize a triangle, I first transform its three vertices into screen space and compute
        an axis-aligned bounding box:
        <code>x_min, x_max, y_min, y_max</code> over the three vertex positions. I clamp this box to
        the framebuffer dimensions so I only consider pixels that could possibly intersect the
        triangle.
      </p>
      <p>
        For each integer pixel <code>(x, y)</code> inside the bounding box, I loop over a uniform
        <code>\(\sqrt{\text{sample\_rate}} \times \sqrt{\text{sample\_rate}}\)</code> grid of
        subsamples. Each subsample has a position
        <code>\((p_x, p_y) = (x + (j + 0.5)/\sqrt{r}, \; y + (i + 0.5)/\sqrt{r})\)</code> where
        <code>r</code> is the sample rate and <code>i, j</code> index into the sub-sample grid.
      </p>
      <p>
        For each subsample, I evaluate three signed edge functions for the directed edges:
      </p>
      <p>
        \[
        E_0 = (x_1 - x_0)(p_y - y_0) - (y_1 - y_0)(p_x - x_0)
        \] \[
        E_1 = (x_2 - x_1)(p_y - y_1) - (y_2 - y_1)(p_x - x_1)
        \] \[
        E_2 = (x_0 - x_2)(p_y - y_2) - (y_0 - y_2)(p_x - x_2)
        \]
      </p>
      <p>
        A subsample is inside the triangle if all three edge values have the same sign (all
        non-negative or all non-positive). If so, I write the triangle’s flat color into that
        subsample entry in my <code>sample_buffer</code>. Later,
        <code>resolve_to_framebuffer</code> averages all subsamples per pixel to produce the final
        pixel color.
      </p>
      <p>
        This algorithm is no worse than checking each sample within the bounding box of the triangle
        because that is exactly what it does: the outer loops visit every pixel in the bounding box
        and the inner loops visit every sub-sample in those pixels, performing one inside-outside
        test per sub-sample. The work is therefore
        <code>O(A * r)</code>, where <code>A</code> is the number of pixels in the bounding box and
        <code>r</code> is the sample rate. I never test samples outside the bounding box and do not
        scan the entire screen, so asymptotically it is equivalent to the naive “check every sample
        in the bounding box” algorithm.
      </p>
      <figure>
        <!-- TODO: replace src with your actual screenshot filename -->
        <img src="images/task1_basic_test4_default.png" alt="basic/test4.svg default view" width="60%" />
        <figcaption>
          basic/test4.svg with default viewing parameters and sample rate 1. The pixel inspector is
          centered on a narrow triangle edge to show aliasing.
        </figcaption>
      </figure>
      <p>
        (Optional extra credit) An optimization direction is to precompute edge function values
        along the leftmost sample on a scanline and update them incrementally as I move in
        <code>x</code>, as well as early-out entire scanlines when all samples are clearly outside.
        In <code>DrawRend::redraw</code> I also have timing hooks to compare a basic path vs an
        optimized path using a flag, which can be used to build a timing comparison table.
      </p>

      <h2>Task 2: Antialiasing by Supersampling</h2>
      <p>
        Instead of storing a single color per pixel, I maintain a <code>sample_buffer</code> of size
        <code>width * height * sample_rate</code>. Conceptually, each pixel contains
        <code>sample_rate</code> subpixels. For points and lines,
        <code>fill_pixel</code> simply fills all samples in that pixel with the same color. For
        triangles, I compute coverage at each sub-sample as described above and write the triangle
        color to only those sub-samples that lie inside.
      </p>
      <p>
        At the end of rasterization, <code>resolve_to_framebuffer</code> averages all subsamples in
        each pixel to get a single final color, which is converted to 8-bit RGB. Changing the sample
        rate is handled by <code>set_sample_rate</code>, which resizes the sample buffer and ensures
        subsequent rasterization uses a denser or coarser grid.
      </p>
      <p>
        Supersampling is useful because it turns a hard yes/no coverage decision into an estimate of
        the coverage fraction inside each pixel. With only one sample at the pixel center, edges are
        very jagged: pixels abruptly switch from triangle color to background as the geometry moves.
        With multiple subsamples, I estimate how much of each pixel is covered and blend accordingly,
        which smooths out high-frequency changes at edges and reduces aliasing.
      </p>
      <p>
        Below are screenshots of <code>basic/test4.svg</code> with default viewing parameters and
        sample rates 1, 4, and 16. The pixel inspector (zoom window) is positioned over the very
        skinny purple triangle corner near the bottom-right to show the effect of supersampling on
        thin geometry and jagged edges.
      </p>
      <figure>
        <table style="width: 100%; text-align: center; border-collapse: collapse">
          <tr>
            <td>
              <img src="images/task2_test4_rate1.png" width="100%" alt="basic/test4.svg, sample rate 1" />
              <figcaption>basic/test4.svg, sample rate 1</figcaption>
            </td>
            <td>
              <img src="images/task2_test4_rate4.png" width="100%" alt="basic/test4.svg, sample rate 4" />
              <figcaption>basic/test4.svg, sample rate 4</figcaption>
            </td>
            <td>
              <img src="images/task2_test4_rate16.png" width="100%" alt="basic/test4.svg, sample rate 16" />
              <figcaption>basic/test4.svg, sample rate 16</figcaption>
            </td>
          </tr>
        </table>
      </figure>
      <p>
        <strong>Why these results are observed:</strong> With <strong>sample rate 1</strong>, each
        pixel has a single sample at its center. The rasterizer makes a binary decision: that
        sample is either inside or outside the triangle. So each pixel is either fully triangle-colored
        or fully background, producing strong <em>stair-stepping (aliasing)</em> along edges. The
        skinny triangle corner is especially jagged because many pixels are only partially covered
        but get a single yes/no result.
      </p>
      <p>
        With <strong>sample rate 4</strong> (a 2×2 grid of subsamples per pixel), we take four
        point-in-triangle tests per pixel and then average their colors. Pixels that are only partly
        covered get a blend of triangle color and background according to how many subsamples hit
        the triangle. That gives a <em>coverage estimate</em> per pixel, so edges look visibly
        smoother and the skinny corner shows intermediate shades instead of hard steps.
      </p>
      <p>
        With <strong>sample rate 16</strong> (4×4 subsamples per pixel), the coverage estimate is
        finer. More subsamples near the true edge fall partly inside and partly outside, so the
        resolved pixel color reflects a more accurate fractional coverage. Edges and the thin
        corner appear much smoother. The tradeoff is higher cost: 16× more samples per pixel and
        proportionally more work in rasterization and resolve.
      </p>
      <figure>
        <img src="images/task2_test4_compare.png" alt="Side-by-side: sample rates 1, 4, 16" width="90%" />
        <figcaption>Side-by-side comparison: basic/test4.svg at sample rates 1, 4, and 16 (pixel inspector on skinny triangle corner).</figcaption>
      </figure>

      <h2>Task 3: Transforms</h2>
      <p>
        The cubeman robot is constructed from rectangles, each with its own transform. Posing him is
        therefore a matter of composing <code>translate</code>, <code>rotate</code>, and
        <code>scale</code> matrices on the body parts. Conceptually, each part has a local
        coordinate frame, and I place it in world space by multiplying the appropriate transforms.
      </p>
      <p>
        In my updated <code>my_robot.svg</code>, I used these transforms to give cubeman a more
        dynamic pose:
      </p>
      <ul>
        <li>
          A global transform slightly tilts his torso forward, so he looks like he is in motion
          instead of standing upright.
        </li>
        <li>
          One arm is rotated up about the shoulder, with a small additional rotation at the elbow,
          to create a clear waving gesture.
        </li>
        <li>
          The legs are translated and rotated so one leg is stepping forward and the other is
          behind, suggesting a walking or running stride.
        </li>
        <li>
          I also made minor color and proportion adjustments (like brighter hands and head) to
          emphasize the motion and make the character stand out.
        </li>
      </ul>
      <p>
        <strong>What I was trying to do with cubeman:</strong> I wanted him to look like he’s
        running toward the viewer while waving hello—a single frozen frame of “hey, I’m coming!”
        So I leaned his whole body forward with a small rotation, put his right arm up with an
        extra bend at the elbow for a clear wave, swung his left arm back and left leg forward
        and right leg back to suggest a mid-stride run. I changed his colors (navy torso, warm
        yellow head, coral limbs and orange arm with a yellow “hand” on the waving arm) and
        slightly narrowed the torso and enlarged the head so he reads more as a friendly character
        in motion rather than a static robot.
      </p>
      <figure>
        <img src="images/task3_my_robot.png" alt="my_robot.svg rendered pose" width="60%" />
        <figcaption>
          My updated cubeman in <code>my_robot.svg</code>, posed to be waving and mid-step using
          combinations of translate, rotate, and scale.
        </figcaption>
      </figure>

      <h2>Task 4: Barycentric coordinates</h2>
      <p>
        Barycentric coordinates describe how much each corner of a triangle
        “contributes” to a point inside it. Instead of using \(x\) and \(y\), points are described 
         by three weights—one per vertex—that sum to 1. The closer the point is to a vertex,
        the higher that vertex’s weight; at a vertex, that weight is 1 and the others are 0. That
        makes it natural to blend vertex attributes (like color) across the triangle: the color at
        any interior point is just the same weighted mix of the three vertex colors. The image
        below shows a single triangle with one red, one green, and one blue vertex; the smooth
        gradient is exactly that barycentric blend.
      </p>
      <figure>
        <img src="images/barycentric_rgb_triangle.png" alt="Single triangle with red, green, and blue vertices; smoothly blended colors from barycentric interpolation." width="50%" />
        <figcaption>
          A single triangle with red, green, and blue vertices (sample rate 1). Interior color is
          the barycentric weighted average of the three vertex colors, producing a smooth blend.
        </figcaption>
      </figure>
      <p>
        Formally, given vertices \(A\), \(B\), and \(C\), any point \(P\) inside the triangle can
        be written as:
      </p>
      <p>
        \[
        P = \alpha A + \beta B + \gamma C,\quad \text{with } \alpha + \beta + \gamma = 1,\; \alpha,
        \beta, \gamma \ge 0.
        \]
      </p>
      <p>
        Geometrically, each weight corresponds to the relative area of the subtriangle opposite that
        vertex. Points closer to vertex \(A\) have a larger \(\alpha\), and similarly for the other
        vertices. Because barycentric coordinates vary linearly across the triangle, they are ideal
        for interpolating vertex attributes such as colors and texture coordinates.
      </p>
      <p>
        In my implementation, I reuse the edge functions already computed for the inside-outside
        test to derive barycentric coordinates at each subsample. Once I have \((\alpha, \beta,
        \gamma)\), I can interpolate either colors (for smooth color gradients) or texture
        coordinates (for texture mapping), simply by taking the corresponding weighted sum.
      </p>
      <figure>
        <img src="images/task4_test7_rate1.png" alt="basic/test7.svg" width="60%" />
        <figcaption>
          basic/test7.svg with sample rate 1. Many small color triangles demonstrate how barycentric
          interpolation produces smooth color transitions across each triangle.
        </figcaption>
      </figure>

      <h2>Task 5: &quot;Pixel sampling&quot; for texture mapping</h2>
      <p>
        <b>Pixel sampling in my own words:</b> The texture is a grid of colored texels, but when we
        map it onto a triangle we get continuous \((u, v)\) coordinates that usually fall
        between texel centers. Pixel sampling is the rule we use to turn that fractional \((u, v)\)
        into a single color. So it’s literally “which texel(s) do we read, and how do we combine
        them?” Different rules give different tradeoffs between sharpness, smoothness, and cost.
      </p>
      <p>
        In my pipeline, once a subsample is inside a textured triangle, I use barycentric
        coordinates to get its \((u, v)\). That plus the current pixel/level sampling mode goes into
        a <code>SampleParams</code>; <code>Texture::sample</code> then dispatches to the right
        routine (nearest or bilinear on the chosen mip level). So texture mapping is: interpolate
        \((u, v)\) per subsample → call pixel sampling to get a color → write that into the
        sample buffer.
      </p>
      <p><b>Two pixel sampling methods:</b></p>
      <ul>
        <li>
          <b>Nearest neighbor (<code>P_NEAREST</code>)</b>: I convert \((u, v)\) into integer texel
          indices by rounding to the nearest texel center and return the color of that texel. This
          is fast and preserves sharp edges in the texture, but it easily produces blocky artifacts
          and aliasing (especially when the texture is minified or magnified).
        </li>
        <li>
          <b>Bilinear interpolation (<code>P_LINEAR</code>)</b>: I find the four texels surrounding
          \((u, v)\), compute fractional distances in the \(u\) and \(v\) directions, and take a
          weighted average of the four texels (first horizontally, then vertically). This smooths
          discontinuities between texels and reduces jaggedness and shimmering, at the cost of
          slightly more computation and a bit of blur.
        </li>
      </ul>
      <p>
        In <code>rasterize_textured_triangle</code>, after confirming a subsample is inside the
        triangle, I compute barycentric coordinates using edge functions, interpolate the three
        vertices’ texture coordinates to a single \((u, v)\), and also compute neighboring
        <code>p_dx_uv</code> and <code>p_dy_uv</code> values by moving one pixel in screen space.
        These derivatives are stored in a <code>SampleParams</code> struct along with the current
        <code>psm</code> and <code>lsm</code>, and passed to <code>Texture::sample</code>.
      </p>
      <p>
        For this task I focus on changing <code>psm</code> between nearest and bilinear while
        holding the level sampling mode fixed (e.g. <code>L_ZERO</code>).
      </p>
      <p>
        <b>Scene and pixel inspector:</b> I used a texture-mapped scene from
        <code>svg/texmap/</code>. <code>texmap/test4.svg</code> and <code>texmap/test1.svg</code>
        both use the parrot texture and are good for comparison. I positioned the pixel
        inspector (zoom window) over a high-detail region—e.g. the parrot’s feathers or
        eye—where the texture has fine variation. There, bilinear sampling clearly defeats
        nearest: nearest shows blocky texels and jagged boundaries, while bilinear gives smooth
        gradients between texels.
      </p>
      <p>
        <b>How to capture the four screenshots:</b> Run the app from <code>build/</code>, load
        <code>svg/texmap/test4.svg</code> (or <code>test1.svg</code>), and enable the pixel
        inspector (zoom). Move the inspector over a detailed part of the parrot (feathers or eye).
        Use <strong>L</strong> to set level sampling to “level zero” so we only compare pixel
        sampling. For each of the four combinations, set the mode then press <strong>S</strong>
        to save (screenshots go to the current working directory, e.g. <code>build/</code>).
        Copy the four PNGs into <code>docs/images/</code> and rename them as below.
      </p>
      <ul>
        <li>Nearest + 1 sample/pixel: <code>P</code> until “nearest pixel”, <code>-</code> until rate 1 → <code>S</code> → save as <code>task5_nearest_1spp.png</code></li>
        <li>Nearest + 16 samples/pixel: same pixel mode, <code>=</code> until rate 16 → <code>S</code> → <code>task5_nearest_16spp.png</code></li>
        <li>Bilinear + 1 sample/pixel: <code>P</code> until “bilinear pixel”, <code>-</code> until rate 1 → <code>S</code> → <code>task5_bilinear_1spp.png</code></li>
        <li>Bilinear + 16 samples/pixel: bilinear, <code>=</code> until rate 16 → <code>S</code> → <code>task5_bilinear_16spp.png</code></li>
      </ul>
      <figure>
        <table style="width: 100%; text-align: center; border-collapse: collapse">
          <tr>
            <td>
              <img src="images/task5_nearest_1spp.png" width="100%" alt="Nearest, 1 sample per pixel" />
              <figcaption>Nearest, 1 sample per pixel</figcaption>
            </td>
            <td>
              <img src="images/task5_nearest_16spp.png" width="100%" alt="Nearest, 16 samples per pixel" />
              <figcaption>Nearest, 16 samples per pixel</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="images/task5_bilinear_1spp.png" width="100%" alt="Bilinear, 1 sample per pixel" />
              <figcaption>Bilinear, 1 sample per pixel</figcaption>
            </td>
            <td>
              <img src="images/task5_bilinear_16spp.png" width="100%" alt="Bilinear, 16 samples per pixel" />
              <figcaption>Bilinear, 16 samples per pixel</figcaption>
            </td>
          </tr>
        </table>
      </figure>
      <p>
        <b>Relative differences:</b> On a high-frequency texture from <code>svg/texmap</code>,
        nearest at 1 sample/pixel looks very blocky—individual texels and strong aliasing
        (moiré, shimmering) are obvious. Nearest at 16 samples/pixel is better because
        supersampling averages multiple samples per pixel near edges, but blockiness between
        texels remains since each subsample still returns a single texel color. Bilinear at 1
        sample/pixel already looks smoother: sharp jumps at texel boundaries become smooth
        transitions. Bilinear at 16 samples/pixel combines both: supersampling reduces geometric
        aliasing and bilinear reduces texture aliasing, so the result is the smoothest of the
        four.
      </p>
      <p>
        <b>When is the difference between nearest and bilinear large?</b> The gap is biggest when
        (1) the texture has high spatial frequency (fine details, thin lines, small text) and
        (2) the texture is minified (one pixel covers many texels) or the sampling rate is low.
        In that regime, nearest sampling makes the chosen texel jump abruptly from one pixel to
        the next, so you see blocky boundaries and moiré. Bilinear sampling blends the four
        neighboring texels, so the color changes smoothly and better respects the sampling
        theorem. When the texture is magnified (one texel spans many pixels) or when the
        texture is very smooth, nearest and bilinear look much closer; the benefit of bilinear
        is most visible in high-detail, minified regions like the parrot’s feathers or eye in
        <code>texmap/test4.svg</code>.
      </p>

      <h2>Task 6: &quot;Level Sampling&quot; with mipmaps for texture mapping</h2>
      <p>
        <b>Level sampling in my own words:</b> When we draw a texture onto the screen, each pixel
        corresponds to some region in texture space—sometimes a tiny patch (magnification), sometimes
        a large patch (minification). Level sampling is the rule for <em>which mip level</em> we read
        from: we have a pyramid of prefiltered, downsampled versions of the texture (mipmaps). If we
        always use the top level (full resolution), we get aliasing when one pixel covers many texels;
        level sampling uses the screen-space derivatives of \((u,v)\) to estimate that footprint, compute
        a continuous mip level \(\ell = \log_2 d\), and then either round to one level (L_NEAREST),
        blend two levels (L_LINEAR), or ignore it and always use level 0 (L_ZERO). So level sampling
        trades a bit of memory and work for much better antialiasing when the texture is minified.
      </p>
      <p>
        <b>Implementation:</b> I estimate the pixel footprint in texture space using derivatives of
        \((u,v)\) with respect to screen \(x\) and \(y\). In <code>Texture::get_level</code> I compute:
      </p>
      <p>
        \[
        d^2_x = (\Delta u / \Delta x)^2 + (\Delta v / \Delta x)^2, \quad
        d^2_y = (\Delta u / \Delta y)^2 + (\Delta v / \Delta y)^2,
        \] \[
        d = \sqrt{\max(d^2_x, d^2_y)}, \quad \text{level} = \log_2 d.
        \]
      </p>
      <p>
        Intuitively, when a pixel covers many texels in texture space, <code>d</code> is large and I
        should use a higher mip level (more blurred, lower resolution) to avoid aliasing. When a
        pixel covers only a small region in texture space, I use a lower mip level (sharper).
      </p>
      <p>I implemented three level sampling modes:</p>
      <ul>
        <li>
          <b><code>L_ZERO</code></b>: always sample from mip level 0 (the original, highest
          resolution texture).
        </li>
        <li>
          <b><code>L_NEAREST</code></b>: round the computed level to the nearest integer and sample
          that single mip level.
        </li>
        <li>
          <b><code>L_LINEAR</code></b>: clamp the level to the valid range, then linearly
          interpolate between the two nearest mip levels (trilinear filtering).
        </li>
      </ul>
      <p>There are important tradeoffs between pixel sampling, level sampling, and supersampling:</p>
      <ul>
        <li>
          <b>Pixel sampling</b> (nearest vs bilinear) affects how colors are combined
          <em>within</em> a given mip level. Nearest is fastest but aliases more; bilinear is
          slightly slower but smoother.
        </li>
        <li>
          <b>Level sampling</b> (L_ZERO, L_NEAREST, L_LINEAR) chooses which mip level(s) to sample.
          Mipmaps add memory overhead (roughly one-third extra), but they dramatically reduce
          aliasing when textures are minified. <code>L_LINEAR</code> smooths transitions between
          mip levels compared to <code>L_NEAREST</code>.
        </li>
        <li>
          <b>Supersampling</b> increases the number of samples per pixel. It is very powerful but
          also very expensive computationally and in sample buffer memory (~sample_rate times more).
          Mipmapping and bilinear filtering are a more targeted and efficient way to handle texture
          aliasing.
        </li>
      </ul>
      <p>
        <b>How the four screenshots were made:</b> I use a custom PNG (a photo with curtains, fabrics,
        and fine detail) in <code>svg/texmap/pexels_test.svg</code> (texture <code>pexels_scene.png</code>).
        You can run <code>./draw</code> with that SVG, then use <strong>L</strong> to cycle level sampling
        (L_ZERO, L_NEAREST, L_LINEAR) and <strong>P</strong> for pixel sampling; press <strong>S</strong> to save
        a screenshot to the current directory. Alternatively, from the repo root run
        <code>./scripts/render_task6_screenshots.sh</code> to render all four combinations headlessly and
        copy them into <code>docs/images/</code>.
      </p>
      <figure>
        <table style="width: 100%; text-align: center; border-collapse: collapse">
          <tr>
            <td>
              <img src="images/task6_L0_Pnearest.png" width="100%" alt="L_ZERO + P_NEAREST" />
              <figcaption>L_ZERO + P_NEAREST</figcaption>
            </td>
            <td>
              <img src="images/task6_L0_Plinear.png" width="100%" alt="L_ZERO + P_LINEAR" />
              <figcaption>L_ZERO + P_LINEAR</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="images/task6_Lnearest_Pnearest.png" width="100%" alt="L_NEAREST + P_NEAREST" />
              <figcaption>L_NEAREST + P_NEAREST</figcaption>
            </td>
            <td>
              <img src="images/task6_Lnearest_Plinear.png" width="100%" alt="L_NEAREST + P_LINEAR" />
              <figcaption>L_NEAREST + P_LINEAR</figcaption>
            </td>
          </tr>
        </table>
      </figure>
      <p>
        Using the custom PNG in <code>pexels_test.svg</code>, <code>L_ZERO + P_NEAREST</code> is the
        fastest but has strong aliasing when the texture is minified: patterns shimmer and exhibit
        moiré. <code>L_ZERO + P_LINEAR</code> smooths texel boundaries but still aliases when there
        is a lot of minification.
      </p>
      <p>
        <code>L_NEAREST + P_NEAREST</code> uses coarser mip levels when appropriate, significantly
        reducing aliasing, but transitions between levels and the blockiness of nearest sampling can
        still show up. <code>L_NEAREST + P_LINEAR</code> gives the best quality overall: mipmaps
        handle minification while bilinear smoothing reduces artifacts within each level and across
        level boundaries. It achieves very good antialiasing with much less cost than very high
        supersampling rates.
      </p>

      <h2>(Optional) Task 7: Extra Credit - Draw Something Creative!</h2>
      <p>
        I saved my best SVG as <code>competition.svg</code> in <code>docs/</code> and rendered it
        at 800×800 using the homework rasterizer (<code>./draw docs/competition.svg</code>, then
        <strong>S</strong> to save a screenshot). Below is the 800×800 PNG screenshot shown in the
        write-up.
      </p>
      <figure>
        <img src="images/competition.svg" width="800" height="800" alt="competition.svg rendered at 800×800" />
        <figcaption>800×800 screenshot of <code>docs/competition.svg</code></figcaption>
      </figure>
      <h3>How I did it</h3>
      <p>
        I wrote a Python script in <code>src/generate_competition_svg.py</code> that procedurally
        generates the SVG. The script outputs only elements the rasterizer supports: solid
        <code>&lt;polygon&gt;</code> triangles, a <code>&lt;texture&gt;</code> declaration, and
        <code>&lt;textri&gt;</code> textured triangles (no paths or curves).
      </p>
      <p>
        <strong>What the script generates:</strong> An 800×800 scene with (1) a two-tone sky
        gradient (four large triangles); (2) concentric spiral arms—several rings of 24 wedge
        triangles each, with radius and angle parameterized by <code>t</code> in [0,1] and colors
        cycling per arm to stress geometric (edge) antialiasing; (3) a tiled diamond pattern—a grid
        of small diamonds (each diamond is four triangles around a center), with alternating row
        offset and color shade varying by cell; (4) three textured quads (each quad as two
        <code>&lt;textri&gt;</code> elements) using different UV regions of the same texture to
        show texture antialiasing; (5) layered hill polygons and a few foreground accent triangles.
      </p>
      <p>
        <strong>How to run it:</strong> From the repo root, run
        <code>python3 src/generate_competition_svg.py</code>. It writes <code>docs/competition.svg</code>.
        Then open that SVG in the rasterizer (e.g. <code>./draw docs/competition.svg</code>), set
        resolution to 800×800 if needed, and press <strong>S</strong> to save the PNG screenshot;
        place that PNG in <code>hw1/images/competition_800x800.png</code> so this write-up can
        display it.
      </p>

		</div>
	</body>
</html>